{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "products=pd.read_csv('C:/Users/gero_/Documents/python works/fuentes/amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive reviews  26579 ||  negative reviews  26493\n"
     ]
    }
   ],
   "source": [
    "count1 =0\n",
    "count2 =0\n",
    "for i in products['sentiment']:\n",
    "    if i >=0:\n",
    "        count1=count1+1\n",
    "    else:\n",
    "        count2=count2+1\n",
    "print('positive reviews ',count1,'||',' negative reviews ',count2)\n",
    "del count1,count2,i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrimos el JSON con las palabras m√°s importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('C:/Users/gero_/Documents/python works/fuentes/important_words.json') as archivo:\n",
    "    important_words=json.load(archivo)\n",
    "archivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baby',\n",
       " 'one',\n",
       " 'great',\n",
       " 'love',\n",
       " 'use',\n",
       " 'would',\n",
       " 'like',\n",
       " 'easy',\n",
       " 'little',\n",
       " 'seat',\n",
       " 'old',\n",
       " 'well',\n",
       " 'get',\n",
       " 'also',\n",
       " 'really',\n",
       " 'son',\n",
       " 'time',\n",
       " 'bought',\n",
       " 'product',\n",
       " 'good',\n",
       " 'daughter',\n",
       " 'much',\n",
       " 'loves',\n",
       " 'stroller',\n",
       " 'put',\n",
       " 'months',\n",
       " 'car',\n",
       " 'still',\n",
       " 'back',\n",
       " 'used',\n",
       " 'recommend',\n",
       " 'first',\n",
       " 'even',\n",
       " 'perfect',\n",
       " 'nice',\n",
       " 'bag',\n",
       " 'two',\n",
       " 'using',\n",
       " 'got',\n",
       " 'fit',\n",
       " 'around',\n",
       " 'diaper',\n",
       " 'enough',\n",
       " 'month',\n",
       " 'price',\n",
       " 'go',\n",
       " 'could',\n",
       " 'soft',\n",
       " 'since',\n",
       " 'buy',\n",
       " 'room',\n",
       " 'works',\n",
       " 'made',\n",
       " 'child',\n",
       " 'keep',\n",
       " 'size',\n",
       " 'small',\n",
       " 'need',\n",
       " 'year',\n",
       " 'big',\n",
       " 'make',\n",
       " 'take',\n",
       " 'easily',\n",
       " 'think',\n",
       " 'crib',\n",
       " 'clean',\n",
       " 'way',\n",
       " 'quality',\n",
       " 'thing',\n",
       " 'better',\n",
       " 'without',\n",
       " 'set',\n",
       " 'new',\n",
       " 'every',\n",
       " 'cute',\n",
       " 'best',\n",
       " 'bottles',\n",
       " 'work',\n",
       " 'purchased',\n",
       " 'right',\n",
       " 'lot',\n",
       " 'side',\n",
       " 'happy',\n",
       " 'comfortable',\n",
       " 'toy',\n",
       " 'able',\n",
       " 'kids',\n",
       " 'bit',\n",
       " 'night',\n",
       " 'long',\n",
       " 'fits',\n",
       " 'see',\n",
       " 'us',\n",
       " 'another',\n",
       " 'play',\n",
       " 'day',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'tried',\n",
       " 'thought',\n",
       " 'never',\n",
       " 'item',\n",
       " 'hard',\n",
       " 'plastic',\n",
       " 'however',\n",
       " 'disappointed',\n",
       " 'reviews',\n",
       " 'something',\n",
       " 'going',\n",
       " 'pump',\n",
       " 'bottle',\n",
       " 'cup',\n",
       " 'waste',\n",
       " 'return',\n",
       " 'amazon',\n",
       " 'different',\n",
       " 'top',\n",
       " 'want',\n",
       " 'problem',\n",
       " 'know',\n",
       " 'water',\n",
       " 'try',\n",
       " 'received',\n",
       " 'sure',\n",
       " 'times',\n",
       " 'chair',\n",
       " 'find',\n",
       " 'hold',\n",
       " 'gate',\n",
       " 'open',\n",
       " 'bottom',\n",
       " 'away',\n",
       " 'actually',\n",
       " 'cheap',\n",
       " 'worked',\n",
       " 'getting',\n",
       " 'ordered',\n",
       " 'came',\n",
       " 'milk',\n",
       " 'bad',\n",
       " 'part',\n",
       " 'worth',\n",
       " 'found',\n",
       " 'cover',\n",
       " 'many',\n",
       " 'design',\n",
       " 'looking',\n",
       " 'weeks',\n",
       " 'say',\n",
       " 'wanted',\n",
       " 'look',\n",
       " 'place',\n",
       " 'purchase',\n",
       " 'looks',\n",
       " 'second',\n",
       " 'piece',\n",
       " 'box',\n",
       " 'pretty',\n",
       " 'trying',\n",
       " 'difficult',\n",
       " 'together',\n",
       " 'though',\n",
       " 'give',\n",
       " 'started',\n",
       " 'anything',\n",
       " 'last',\n",
       " 'company',\n",
       " 'come',\n",
       " 'returned',\n",
       " 'maybe',\n",
       " 'took',\n",
       " 'broke',\n",
       " 'makes',\n",
       " 'stay',\n",
       " 'instead',\n",
       " 'idea',\n",
       " 'head',\n",
       " 'said',\n",
       " 'less',\n",
       " 'went',\n",
       " 'working',\n",
       " 'high',\n",
       " 'unit',\n",
       " 'seems',\n",
       " 'picture',\n",
       " 'completely',\n",
       " 'wish',\n",
       " 'buying',\n",
       " 'babies',\n",
       " 'won',\n",
       " 'tub',\n",
       " 'almost',\n",
       " 'either']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text:str):\n",
    "   import string\n",
    "   for i in text:\n",
    "      if i in string.punctuation:\n",
    "         text=text.replace(i,'')\n",
    "         \n",
    "   return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['review_clean']=products['review'].apply(lambda x :remove_punctuation(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.loc[0,'review_clean'].split().count('of')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n",
      "C:\\Users\\gero_\\AppData\\Local\\Temp\\ipykernel_10116\\2345936920.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  products[word]=products['review_clean'].apply(lambda x : x.split().count(word))\n"
     ]
    }
   ],
   "source": [
    "for word in important_words:\n",
    "    products[word]=products['review_clean'].apply(lambda x : x.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>baby</th>\n",
       "      <th>one</th>\n",
       "      <th>great</th>\n",
       "      <th>love</th>\n",
       "      <th>use</th>\n",
       "      <th>...</th>\n",
       "      <th>seems</th>\n",
       "      <th>picture</th>\n",
       "      <th>completely</th>\n",
       "      <th>wish</th>\n",
       "      <th>buying</th>\n",
       "      <th>babies</th>\n",
       "      <th>won</th>\n",
       "      <th>tub</th>\n",
       "      <th>almost</th>\n",
       "      <th>either</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>All of my kids have cried nonstop when I tried...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>We wanted to get something to keep track of ou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nature's Lullabies Second Year Sticker Calendar</td>\n",
       "      <td>My daughter had her 1st baby over a year ago. ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>My daughter had her 1st baby over a year ago S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lamaze Peekaboo, I Love You</td>\n",
       "      <td>One of baby's first and favorite books, and it...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>One of babys first and favorite books and it i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SoftPlay Peek-A-Boo Where's Elmo A Children's ...</td>\n",
       "      <td>Very cute interactive book! My son loves this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Very cute interactive book My son loves this b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Beautiful book, I love it to record cherished ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Beautiful book I love it to record cherished t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hunnt&amp;reg; Falling Flowers and Birds Kids Nurs...</td>\n",
       "      <td>Try this out for a spring project !Easy ,fun a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Try this out for a spring project Easy fun and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blessed By Pope Benedict XVI Divine Mercy Full...</td>\n",
       "      <td>very nice Divine Mercy Pendant of Jesus now on...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>very nice Divine Mercy Pendant of Jesus now on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cloth Diaper Pins Stainless Steel Traditional ...</td>\n",
       "      <td>We bought the pins as my 6 year old Autistic s...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>We bought the pins as my 6 year old Autistic s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cloth Diaper Pins Stainless Steel Traditional ...</td>\n",
       "      <td>It has been many years since we needed diaper ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>It has been many years since we needed diaper ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Stop Pacifier Sucking without tears with Thumb...   \n",
       "1    Nature's Lullabies Second Year Sticker Calendar   \n",
       "2    Nature's Lullabies Second Year Sticker Calendar   \n",
       "3                        Lamaze Peekaboo, I Love You   \n",
       "4  SoftPlay Peek-A-Boo Where's Elmo A Children's ...   \n",
       "5                          Our Baby Girl Memory Book   \n",
       "6  Hunnt&reg; Falling Flowers and Birds Kids Nurs...   \n",
       "7  Blessed By Pope Benedict XVI Divine Mercy Full...   \n",
       "8  Cloth Diaper Pins Stainless Steel Traditional ...   \n",
       "9  Cloth Diaper Pins Stainless Steel Traditional ...   \n",
       "\n",
       "                                              review  rating  sentiment  \\\n",
       "0  All of my kids have cried non-stop when I trie...       5          1   \n",
       "1  We wanted to get something to keep track of ou...       5          1   \n",
       "2  My daughter had her 1st baby over a year ago. ...       5          1   \n",
       "3  One of baby's first and favorite books, and it...       4          1   \n",
       "4  Very cute interactive book! My son loves this ...       5          1   \n",
       "5  Beautiful book, I love it to record cherished ...       5          1   \n",
       "6  Try this out for a spring project !Easy ,fun a...       5          1   \n",
       "7  very nice Divine Mercy Pendant of Jesus now on...       5          1   \n",
       "8  We bought the pins as my 6 year old Autistic s...       4          1   \n",
       "9  It has been many years since we needed diaper ...       5          1   \n",
       "\n",
       "                                        review_clean  baby  one  great  love  \\\n",
       "0  All of my kids have cried nonstop when I tried...     0    0      1     0   \n",
       "1  We wanted to get something to keep track of ou...     0    0      0     0   \n",
       "2  My daughter had her 1st baby over a year ago S...     1    0      0     0   \n",
       "3  One of babys first and favorite books and it i...     0    0      0     0   \n",
       "4  Very cute interactive book My son loves this b...     0    0      1     0   \n",
       "5  Beautiful book I love it to record cherished t...     0    0      1     1   \n",
       "6  Try this out for a spring project Easy fun and...     0    0      0     0   \n",
       "7  very nice Divine Mercy Pendant of Jesus now on...     0    0      0     0   \n",
       "8  We bought the pins as my 6 year old Autistic s...     0    1      0     0   \n",
       "9  It has been many years since we needed diaper ...     0    1      0     0   \n",
       "\n",
       "   use  ...  seems  picture  completely  wish  buying  babies  won  tub  \\\n",
       "0    0  ...      0        0           0     0       0       0    0    0   \n",
       "1    0  ...      0        0           0     0       0       0    0    0   \n",
       "2    0  ...      0        0           0     0       0       0    0    0   \n",
       "3    0  ...      0        0           0     0       0       0    0    0   \n",
       "4    0  ...      0        0           0     0       0       1    0    0   \n",
       "5    0  ...      0        0           0     0       0       0    0    0   \n",
       "6    0  ...      0        0           0     0       0       0    0    0   \n",
       "7    0  ...      0        0           0     0       0       0    0    0   \n",
       "8    1  ...      0        0           0     0       0       0    0    0   \n",
       "9    0  ...      0        0           0     0       0       0    0    0   \n",
       "\n",
       "   almost  either  \n",
       "0       0       0  \n",
       "1       0       0  \n",
       "2       0       0  \n",
       "3       0       0  \n",
       "4       0       0  \n",
       "5       0       0  \n",
       "6       0       0  \n",
       "7       0       0  \n",
       "8       0       0  \n",
       "9       0       0  \n",
       "\n",
       "[10 rows x 198 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de reviews con la palabra perfect  2955\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in products['perfect']:\n",
    "    if i >=1:\n",
    "        count=count+1\n",
    "print('cantidad de reviews con la palabra perfect ',count)\n",
    "del count,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe,features,target):\n",
    "    dataframe['constant']=1\n",
    "    features=['constant']+features\n",
    "    features_temp=dataframe[features]\n",
    "    features_matrix=np.matrix(features_temp)\n",
    "\n",
    "    df_target=dataframe[target]\n",
    "    arr_target=np.matrix(df_target)\n",
    "    return (features_matrix,arr_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cantidad de filas y columnas\n",
    "np.shape(get_numpy_data(products,important_words,'sentiment')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 1, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_numpy_data(products,important_words,'sentiment')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability (feature_matrix,weights):\n",
    "    score=np.matrix(feature_matrix)*np.transpose(np.matrix(weights))\n",
    "    #devuelve una matriz [D,1]\n",
    "    return 1/( 1+ np.exp( -score ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative (errors,feature):\n",
    "    derivative= np.matrix(errors)*np.transpose(np.matrix(feature))\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator=np.transpose(np.matrix(np.where(np.array(sentiment)>0 ,1,0) ))\n",
    "    scores = np.matrix(feature_matrix)*np.transpose(np.matrix(coefficients))\n",
    "    #func1=np.vectorize(lambda x : np.log(1+ np.exp(-x)))\n",
    "    #lp=np.multiply(indicator-1,scores)-func1(scores)\n",
    "    lp=np.multiply(indicator-1,scores)-np.log(1+ np.exp(-scores))\n",
    "    # compute_log_likelihood([[1 ,2 ,3],[3 ,4 ,5]],[1,-1],[4,5,6])\n",
    "    return np.sum(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients=np.matrix(initial_coefficients)\n",
    "    for itr in range(max_iter):\n",
    "        predictions=predict_probability(feature_matrix,coefficients)\n",
    "        indicator=np.transpose(np.matrix(np.where(np.array(sentiment)>0 ,1,0) ))\n",
    "        errors=indicator-predictions\n",
    "\n",
    "        derivative=np.transpose(feature_matrix)*errors #matiz[D,1]\n",
    "        coefficients=coefficients + step_size*np.transpose(derivative)\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix,sentiment=get_numpy_data(products,important_words,'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weight=np.zeros(194)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13434712\n",
      "iteration   2: log likelihood of observed labels = -36769.35713564\n",
      "iteration   3: log likelihood of observed labels = -36763.58603240\n",
      "iteration   4: log likelihood of observed labels = -36757.82101962\n",
      "iteration   5: log likelihood of observed labels = -36752.06207964\n",
      "iteration   6: log likelihood of observed labels = -36746.30919497\n",
      "iteration   7: log likelihood of observed labels = -36740.56234821\n",
      "iteration   8: log likelihood of observed labels = -36734.82152213\n",
      "iteration   9: log likelihood of observed labels = -36729.08669961\n",
      "iteration  10: log likelihood of observed labels = -36723.35786366\n",
      "iteration  11: log likelihood of observed labels = -36717.63499744\n",
      "iteration  12: log likelihood of observed labels = -36711.91808422\n",
      "iteration  13: log likelihood of observed labels = -36706.20710739\n",
      "iteration  14: log likelihood of observed labels = -36700.50205049\n",
      "iteration  15: log likelihood of observed labels = -36694.80289716\n",
      "iteration  20: log likelihood of observed labels = -36666.39512033\n",
      "iteration  30: log likelihood of observed labels = -36610.01327118\n",
      "iteration  40: log likelihood of observed labels = -36554.19728365\n",
      "iteration  50: log likelihood of observed labels = -36498.93316099\n",
      "iteration  60: log likelihood of observed labels = -36444.20783914\n",
      "iteration  70: log likelihood of observed labels = -36390.00909449\n",
      "iteration  80: log likelihood of observed labels = -36336.32546144\n",
      "iteration  90: log likelihood of observed labels = -36283.14615871\n",
      "iteration 100: log likelihood of observed labels = -36230.46102347\n",
      "iteration 200: log likelihood of observed labels = -35728.89418769\n",
      "iteration 300: log likelihood of observed labels = -35268.51212683\n"
     ]
    }
   ],
   "source": [
    "coefficients=logistic_regression(feature_matrix,sentiment,initial_weight,1e-7,301)\n",
    "scores = feature_matrix*np.transpose(coefficients)\n",
    "predictions=np.where(scores >0, 1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Midiendo la precisi√≥n del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_cal(predicciones,target):\n",
    "    cont=0\n",
    "    for i,j in zip(predicciones,target):\n",
    "        if i==j:\n",
    "            cont=cont+1\n",
    "    print('predicciones correctas',cont)\n",
    "    print (cont/len(predicciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicciones correctas 39903\n",
      "0.7518653904130238\n"
     ]
    }
   ],
   "source": [
    "accuracy_cal(predictions,products['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que palabras contribuyen m√°s en los sentimientos positivos o negativos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigth_words(significant_words,weights,order):\n",
    "    words_coef= dict()\n",
    "    for ii,jj in zip(significant_words,weights):\n",
    "        words_coef[ii]=jj\n",
    "    if order=='asc':\n",
    "        return sorted(words_coef.items(), key=lambda item: item[1],reverse=False)\n",
    "    elif order=='desc':\n",
    "        return sorted(words_coef.items(), key=lambda item: item[1],reverse=True)\n",
    "    else:\n",
    "        return print('Ingrese el parametro orden correctamente')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las 10 palabras m√°s positivas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', matrix([[0.06654608]])),\n",
       " ('love', matrix([[0.06589076]])),\n",
       " ('easy', matrix([[0.06479459]])),\n",
       " ('little', matrix([[0.04543563]])),\n",
       " ('loves', matrix([[0.0449764]])),\n",
       " ('well', matrix([[0.030135]])),\n",
       " ('perfect', matrix([[0.02973994]])),\n",
       " ('old', matrix([[0.02007754]])),\n",
       " ('nice', matrix([[0.01840871]])),\n",
       " ('daughter', matrix([[0.0177032]]))]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth_words(important_words,np.transpose(coefficients)[1:],'desc')[:10]\n",
    "#tener en cuenta no tomar el primer coeficiente ya que corresponde al termino independiente "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las 10 palabras m√°s negativas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('would', matrix([[-0.05386015]])),\n",
       " ('product', matrix([[-0.04151103]])),\n",
       " ('money', matrix([[-0.03898204]])),\n",
       " ('work', matrix([[-0.03306952]])),\n",
       " ('even', matrix([[-0.03005125]])),\n",
       " ('disappointed', matrix([[-0.02897898]])),\n",
       " ('get', matrix([[-0.02871155]])),\n",
       " ('back', matrix([[-0.0277427]])),\n",
       " ('return', matrix([[-0.02659278]])),\n",
       " ('monitor', matrix([[-0.0244821]]))]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weigth_words(important_words,np.transpose(coefficients)[1:],'asc')[:10]\n",
    "#tener en cuenta no tomar el primer coeficiente ya que corresponde al termino independiente"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
